{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Embedding Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up DB First"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB URL:  postgresql://postgres:cunytechprep@localhost:5432/mydb\n",
      "Connected\n",
      "All tables created\n",
      "Connection successful\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine, Column, Integer, String, ARRAY, Float, text, TIMESTAMP, ForeignKey\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.dialects.postgresql import UUID as DB_UUID\n",
    "import uuid\n",
    "import os\n",
    "from sqlalchemy.orm import sessionmaker, declarative_base, Session\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel\n",
    "from typing import Optional\n",
    "from uuid import UUID\n",
    "import psycopg2\n",
    "from pgvector.sqlalchemy import Vector\n",
    "from sqlalchemy import select\n",
    "\n",
    "# Create the sqlachemy engine and connect to our db\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "DATABASE_URL = os.getenv(\"DATABASE_URL\")\n",
    "print(\"DB URL: \", DATABASE_URL)\n",
    "\n",
    "try:\n",
    "    engine = create_engine(DATABASE_URL)\n",
    "    print(\"Connected\")\n",
    "except Exception as e:\n",
    "    print(\"Connection falied: \", e)\n",
    "\n",
    "# Create a session\n",
    "# auto commmit, transactions are not auto commited, need to call session.commit()\n",
    "# auto flush, changes are not automatically written to db before every query, need to call session.flush()\n",
    "# bind to the db engine we created\n",
    "SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n",
    "\n",
    "# Base class for declarative models\n",
    "# Any class that inherits from this will be recognized by SQLAlchemy as a database table\n",
    "Base = declarative_base()\n",
    "\n",
    "# SQLAlchemy Data models\n",
    "\n",
    "class ContentAI(Base):\n",
    "    __tablename__ = \"content_ai\"\n",
    "\n",
    "    content_id = Column(DB_UUID, ForeignKey(\"content.content_id\"), primary_key=True)\n",
    "    ai_summary = Column(String, nullable=True)\n",
    "    embedding = Column(Vector(dim=384), nullable=True)\n",
    "    # embedding = Column(String, nullable=True) # pgvector integration may need different type (OLD CODE)\n",
    "\n",
    "    # NEED TO REPLACE DIMENSIONS WITH CORRECT EMBEDDING MODEL\n",
    "    # https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n",
    "    # For example, this transformer has 384 dim dense vector\n",
    "    # I am using vector w/ dim of 2 for now \n",
    "    \n",
    "class ContentItem(Base):\n",
    "    __tablename__ = \"content_item\"\n",
    "\n",
    "    user_id = Column(DB_UUID, ForeignKey(\"users.id\"), primary_key=True)\n",
    "    content_id = Column(DB_UUID, ForeignKey(\"content.content_id\"), primary_key=True)\n",
    "    saved_at = Column(TIMESTAMP, server_default=\"NOW()\")\n",
    "    notes = Column(String, nullable=True)\n",
    "\n",
    "class Content(Base):\n",
    "    __tablename__ = \"content\"\n",
    "\n",
    "    content_id = Column(DB_UUID, primary_key=True, default=uuid.uuid4)\n",
    "    user_id = Column(DB_UUID, ForeignKey(\"users.id\"))\n",
    "    url = Column(String, unique=True, nullable=False)   \n",
    "    title = Column(String, nullable=True)\n",
    "    source = Column(String, nullable=True)\n",
    "    first_saved_at = Column(TIMESTAMP, server_default=\"NOW()\")\n",
    "\n",
    "class User(Base):\n",
    "    __tablename__ = \"users\"\n",
    "\n",
    "    id = Column(DB_UUID, primary_key=True, default=uuid)\n",
    "    email = Column(String, unique=True, nullable=False)\n",
    "    created_at = Column(TIMESTAMP, server_default=\"NOW()\")\n",
    "\n",
    "# Schemas content, these are pydantic schemas for data val and serialization\n",
    "\n",
    "class ContentCreate(BaseModel):\n",
    "    url: str\n",
    "    title: Optional[str] = None\n",
    "    source: Optional[str] = None\n",
    "\n",
    "\n",
    "class ContentRead(ContentCreate):\n",
    "    content_id: UUID\n",
    "\n",
    "    class Config:\n",
    "        from_attributes=True\n",
    "\n",
    "# Create tables for the sqlalchemy models defined above, only creates tables that do not exist\n",
    "\n",
    "Base.metadata.create_all(bind=engine)\n",
    "print(\"All tables created\")\n",
    "\n",
    "# Database.py\n",
    "\n",
    "# yield a fresh session per request (FASTAPI), caller to use the session and closes when done w/ session\n",
    "def get_db():\n",
    "    # session instance\n",
    "    db = SessionLocal()  \n",
    "    try:\n",
    "        yield db         \n",
    "    finally:\n",
    "        db.close()\n",
    "\n",
    "# test connection\n",
    "\n",
    "try:\n",
    "    conn = psycopg2.connect(\n",
    "        dbname=\"mydb\",\n",
    "        user=\"postgres\",\n",
    "        password=\"cunytechprep\",\n",
    "        host=\"localhost\",\n",
    "        port=\"5432\"\n",
    "    )\n",
    "    print(\"Connection successful\")\n",
    "    conn.close()    \n",
    "except Exception as e:\n",
    "    print(\"Connection failed:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing all-mini sentence transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "'''\n",
    "https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n",
    "all-MiniLM-L6-v2: This is a very popular model that provides a good balance of speed, size (relatively small), and accuracy for general-purpose semantic search.   \n",
    "all-mpnet-base-v2: Generally offers higher accuracy than all-MiniLM-L6-v2 but is larger and slightly slower.\n",
    "multi-qa-mpnet-base-dot-v1 / multi-qa-MiniLM-L6-cos-v1: These models are specifically trained for question-answering and retrieval tasks and can perform well for semantic search where the queries are question-like.   \n",
    "paraphrase-multilingual-mpnet-base-v2 / paraphrase-multilingual-MiniLM-L12-v2: Excellent choices if you need to support multiple languages.\n",
    "'''\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI Generated sentences to embed, will be searched through\n",
    "sentences = [\n",
    "    \"This is a site about dogs.\",\n",
    "    \"Youtube video of cute puppies playing in the park.\",\n",
    "    \"A website providing information on sustainable living practices.\",\n",
    "    \"Blog with vegan dessert recipes.\",\n",
    "    \"Official online store for handcrafted leather goods.\",\n",
    "    \"News and articles on artificial intelligence.\",\n",
    "    \"Community forum for vintage motorcycle enthusiasts.\",\n",
    "    \"Portfolio showcasing freelance web development work.\",\n",
    "    \"Learn about the history of ancient Egypt.\",\n",
    "    \"Platform for booking guided tours.\",\n",
    "    \"Resources for learning the Spanish language.\",\n",
    "    \"Reviews of the best hiking gear.\",\n",
    "    \"Online tool for converting image formats.\",\n",
    "    \"Site to create and share online surveys.\",\n",
    "    \"Platform connecting pet owners with local sitters.\",\n",
    "    \"Download free stock photos and videos here.\",\n",
    "    \"Register for the upcoming tech conference.\",\n",
    "    \"Online marketplace for used books.\",\n",
    "    \"Track your fitness progress and set goals.\",\n",
    "    \"Service for generating creative writing prompts.\",\n",
    "    \"Real-time updates on cryptocurrency prices.\",\n",
    "    \"Platform for collaborating on software projects.\",\n",
    "    \"Blog where developers share coding tips.\",\n",
    "    \"Online shop for ethically sourced coffee beans.\",\n",
    "    \"Forum for gardeners to discuss plant care.\",\n",
    "    \"Website offering online data science courses.\",\n",
    "    \"Platform for artists to showcase and sell artwork.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content with URL 'http://example.com/0' already exists. Skipping insertion.\n",
      "Content with URL 'http://example.com/1' already exists. Skipping insertion.\n",
      "Content with URL 'http://example.com/2' already exists. Skipping insertion.\n",
      "Content with URL 'http://example.com/3' already exists. Skipping insertion.\n",
      "Content with URL 'http://example.com/4' already exists. Skipping insertion.\n",
      "Content with URL 'http://example.com/5' already exists. Skipping insertion.\n",
      "Content with URL 'http://example.com/6' already exists. Skipping insertion.\n",
      "Content with URL 'http://example.com/7' already exists. Skipping insertion.\n",
      "Content with URL 'http://example.com/8' already exists. Skipping insertion.\n",
      "Content with URL 'http://example.com/9' already exists. Skipping insertion.\n",
      "Content with URL 'http://example.com/10' already exists. Skipping insertion.\n",
      "Content with URL 'http://example.com/11' already exists. Skipping insertion.\n",
      "Content with URL 'http://example.com/12' already exists. Skipping insertion.\n",
      "Content with URL 'http://example.com/13' already exists. Skipping insertion.\n",
      "Content with URL 'http://example.com/14' already exists. Skipping insertion.\n",
      "Content with URL 'http://example.com/15' already exists. Skipping insertion.\n",
      "Content with URL 'http://example.com/16' already exists. Skipping insertion.\n",
      "Content with URL 'http://example.com/17' already exists. Skipping insertion.\n",
      "Content with URL 'http://example.com/18' already exists. Skipping insertion.\n",
      "Content with URL 'http://example.com/19' already exists. Skipping insertion.\n",
      "Content with URL 'http://example.com/20' already exists. Skipping insertion.\n",
      "Content with URL 'http://example.com/21' already exists. Skipping insertion.\n",
      "Content with URL 'http://example.com/22' already exists. Skipping insertion.\n",
      "Content with URL 'http://example.com/23' already exists. Skipping insertion.\n",
      "Content with URL 'http://example.com/24' already exists. Skipping insertion.\n",
      "Content with URL 'http://example.com/25' already exists. Skipping insertion.\n",
      "Content with URL 'http://example.com/26' already exists. Skipping insertion.\n"
     ]
    }
   ],
   "source": [
    "# I am loosely defining this function for now and will clean it up\n",
    "\n",
    "def create_content_with_embedding(db, content_data, text):\n",
    "\n",
    "    # Check if this urls exists\n",
    "    url_to_check = content_data.get(\"url\")\n",
    "    if url_to_check:\n",
    "        existing_content = db.scalar(select(Content).where(Content.url == url_to_check))\n",
    "        if existing_content:\n",
    "            print(f\"Content with URL '{url_to_check}' already exists. Skipping insertion.\")\n",
    "            return existing_content, None  \n",
    "\n",
    "    # Add the content data to the db\n",
    "    content = Content(**content_data)\n",
    "    db.add(content)\n",
    "    db.commit()\n",
    "    db.refresh(content)\n",
    "\n",
    "    #### TODO ########################################################################\n",
    "\n",
    "    # Use an LLM to summarize the url\n",
    "    url = content.url\n",
    "    title = content.title\n",
    "    # source = content.source\n",
    "    # print(f\"The following will be given to the LLM to be summarized {url}, {title}\")\n",
    "    ai_summary = text\n",
    "\n",
    "    ##################################################################################\n",
    "    \n",
    "    # Embed the ai summary text\n",
    "    embedding_vector = model.encode(text) # NEED TO CHANGE BACK TO AI SUMMARY\n",
    "    content_ai = ContentAI(content_id=content.content_id, ai_summary=ai_summary, embedding=embedding_vector)\n",
    "    db.add(content_ai)\n",
    "    db.commit()\n",
    "    db.refresh(content_ai)\n",
    "\n",
    "    print(f\"Created Content ID: {content.content_id},\\n Content AI ID: {content_ai.content_id},\\n Embedding (first 10): {content_ai.embedding[:10]}\\n\\n\")\n",
    "\n",
    "    return content, content_ai\n",
    "\n",
    "\n",
    "# Call the generator function on a new sessions, fresh session per request FASTAPI\n",
    "db = next(get_db())\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    content_data = {\"url\": f\"http://example.com/{i}\", \"title\": f\"Document {i}\"}\n",
    "    create_content_with_embedding(db, content_data, sentence)\n",
    "\n",
    "\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am querying: work out\n",
      "\n",
      "This query embedding (First 10) = [-0.06827506  0.05004624 -0.01302164  0.05232676 -0.05089933  0.01155781\n",
      "  0.07433058 -0.00416497  0.01187012 -0.02499063]\n",
      "\n",
      "Similar Content --------------------\n",
      "\n",
      "AI SUMMARY: Track your fitness progress and set goals.,\n",
      " Content ID: 804c4a6a-505d-4477-a32b-22f043a865ed,\n",
      " Title: Document 18,\n",
      " Embedding (first 10): [ 0.00915601  0.04066574 -0.02963619  0.0440957   0.0350326   0.05192318\n",
      " -0.01645158 -0.05470086 -0.05029801 -0.07216614]\n",
      "\n",
      "\n",
      "AI SUMMARY: Reviews of the best hiking gear.,\n",
      " Content ID: e1a87727-7c4d-4bfb-b361-981b8ec80d65,\n",
      " Title: Document 11,\n",
      " Embedding (first 10): [-0.0903054   0.02331327  0.05094973  0.06554957  0.00227929 -0.03645795\n",
      "  0.05474317  0.00542936 -0.09139992  0.08928707]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Vector class https://github.com/pgvector/pgvector-python/blob/master/pgvector/sqlalchemy/vector.py#L43\n",
    "\n",
    "def find_similar_content(db, query_text, limit=2):\n",
    "    \n",
    "    query_embedding = model.encode(query_text) \n",
    "    print(f\"This query embedding (First 10) = {query_embedding[:10]}\")\n",
    "\n",
    "    results = db.query(ContentAI, Content) \\\n",
    "        .join(Content, ContentAI.content_id == Content.content_id) \\\n",
    "        .order_by(ContentAI.embedding.l2_distance(query_embedding)) \\\n",
    "        .limit(limit) \\\n",
    "        .all()\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "db = next(get_db())\n",
    "\n",
    "# Similarity Search\n",
    "query = \"work out\"\n",
    "print(f\"I am querying: {query}\\n\")\n",
    "similar_results = find_similar_content(db, query)\n",
    "\n",
    "print(\"\\nSimilar Content --------------------\\n\")\n",
    "for content_ai, content in similar_results:\n",
    "    print(f\"AI SUMMARY: {content_ai.ai_summary},\\n \"f\"Content ID: {content.content_id},\\n Title: {content.title},\\n Embedding (first 10): {content_ai.embedding[:10]}\\n\\n\")\n",
    "\n",
    "db.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
