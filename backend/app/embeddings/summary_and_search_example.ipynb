{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB URL:  postgresql://postgres:cunytechprep@localhost:5432/mydb\n",
      "Connected\n",
      "All tables created\n",
      "Connection successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/pydantic/_internal/_config.py:341: UserWarning: Valid config keys have changed in V2:\n",
      "* 'orm_mode' has been renamed to 'from_attributes'\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine, Column, Integer, String, ARRAY, Float, text, TIMESTAMP, ForeignKey\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.dialects.postgresql import UUID as DB_UUID\n",
    "import uuid\n",
    "import os\n",
    "from sqlalchemy.orm import sessionmaker, declarative_base, Session\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel\n",
    "from typing import Optional\n",
    "from uuid import UUID\n",
    "import psycopg2\n",
    "from pgvector.sqlalchemy import Vector\n",
    "from sqlalchemy import select\n",
    "from sqlalchemy import Column, String, TIMESTAMP\n",
    "from sqlalchemy.dialects.postgresql import UUID\n",
    "from pydantic import BaseModel, EmailStr\n",
    "from datetime import datetime\n",
    "\n",
    "# Create the sqlachemy engine and connect to our db\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "DATABASE_URL = os.getenv(\"DATABASE_URL\")\n",
    "print(\"DB URL: \", DATABASE_URL)\n",
    "\n",
    "try:\n",
    "    engine = create_engine(DATABASE_URL)\n",
    "    print(\"Connected\")\n",
    "except Exception as e:\n",
    "    print(\"Connection falied: \", e)\n",
    "\n",
    "# Create a session\n",
    "# auto commmit, transactions are not auto commited, need to call session.commit()\n",
    "# auto flush, changes are not automatically written to db before every query, need to call session.flush()\n",
    "# bind to the db engine we created\n",
    "SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n",
    "\n",
    "# Base class for declarative models\n",
    "# Any class that inherits from this will be recognized by SQLAlchemy as a database table\n",
    "Base = declarative_base()\n",
    "\n",
    "# SQLAlchemy Data models\n",
    "\n",
    "class ContentAI(Base):\n",
    "    __tablename__ = \"content_ai\"\n",
    "\n",
    "    content_id = Column(UUID(as_uuid=True), ForeignKey(\"content.content_id\"), primary_key=True)\n",
    "    ai_summary = Column(String, nullable=True)\n",
    "    embedding = Column(Vector(dim=384), nullable=True)\n",
    "    \n",
    "class ContentItem(Base):\n",
    "    __tablename__ = \"content_item\"\n",
    "\n",
    "    user_id = Column(UUID(as_uuid=True), ForeignKey(\"users.id\"), primary_key=True)\n",
    "    content_id = Column(UUID(as_uuid=True), ForeignKey(\"content.content_id\"), primary_key=True)\n",
    "    saved_at = Column(TIMESTAMP, server_default=\"NOW()\")\n",
    "    notes = Column(String, nullable=True)\n",
    "\n",
    "class Content(Base):\n",
    "    __tablename__ = \"content\"\n",
    "\n",
    "    content_id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)\n",
    "    user_id = Column(UUID(as_uuid=True), ForeignKey(\"users.id\"))\n",
    "    url = Column(String, unique=True, nullable=False)   \n",
    "    title = Column(String, nullable=True)\n",
    "    source = Column(String, nullable=True)\n",
    "    first_saved_at = Column(TIMESTAMP, server_default=\"NOW()\")\n",
    "\n",
    "class User(Base):\n",
    "    __tablename__ = \"users\"\n",
    "\n",
    "    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid)\n",
    "    email = Column(String, unique=True, nullable=False)\n",
    "    created_at = Column(TIMESTAMP, server_default=\"NOW()\")\n",
    "    username = Column(String,  nullable=False)\n",
    "    password = Column(String, nullable=False)\n",
    "\n",
    "\n",
    "\n",
    "class UserCreate(BaseModel):\n",
    "    email: EmailStr  # email field is validated as a proper email format\n",
    "    created_at: datetime = None  # Optional: you can default this to now on the server-side\n",
    "\n",
    "    class Config:\n",
    "        orm_mode = True\n",
    "\n",
    "# Schemas content, these are pydantic schemas for data val and serialization\n",
    "\n",
    "# class ContentCreate(BaseModel):\n",
    "#     url: str\n",
    "#     title: Optional[str] = None\n",
    "#     source: Optional[str] = None\n",
    "\n",
    "\n",
    "# class ContentRead(ContentCreate):\n",
    "#     content_id: UUID\n",
    "\n",
    "#     class Config:\n",
    "#         from_attributes=True\n",
    "\n",
    "# Create tables for the sqlalchemy models defined above, only creates tables that do not exist\n",
    "\n",
    "Base.metadata.create_all(bind=engine)\n",
    "print(\"All tables created\")\n",
    "\n",
    "# Database.py\n",
    "\n",
    "# yield a fresh session per request (FASTAPI), caller to use the session and closes when done w/ session\n",
    "def get_db():\n",
    "    # session instance\n",
    "    db = SessionLocal()  \n",
    "    try:\n",
    "        yield db         \n",
    "    finally:\n",
    "        db.close()\n",
    "\n",
    "# test connection\n",
    "\n",
    "try:\n",
    "    conn = psycopg2.connect(\n",
    "        dbname=\"mydb\",\n",
    "        user=\"postgres\",\n",
    "        password=\"cunytechprep\",\n",
    "        host=\"localhost\",\n",
    "        port=\"5432\"\n",
    "    )\n",
    "    print(\"Connection successful\")\n",
    "    conn.close()    \n",
    "except Exception as e:\n",
    "    print(\"Connection failed:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "# from app.data_models.content_ai import ContentAI\n",
    "# from app.data_models.content import Content\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "from sqlalchemy.orm import Session\n",
    "from transformers import pipeline\n",
    "from readability import Document\n",
    "from bs4 import BeautifulSoup\n",
    "from sqlalchemy import select\n",
    "from uuid import UUID\n",
    "import requests\n",
    "\n",
    "\n",
    "class ContentEmbeddingManager:\n",
    "    '''\n",
    "    Manages:\n",
    "        - Generating vector embeddings for content summaries\n",
    "        - Inserting and retrieving content and their embeddings from the db\n",
    "        - Enriching raw HTML content for a summarization model\n",
    "        - Performing similarity queries on content embeddings\n",
    "        - Handling database interactions for both `Content` and `ContentAI` models\n",
    "    '''\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            db, \n",
    "            embedding_model_name='sentence-transformers/all-MiniLM-L6-v2', \n",
    "            summary_model_name='google-t5/t5-small' # We can always change the model\n",
    "    ):\n",
    "        self.db = db\n",
    "        self.embedding_model = SentenceTransformer(embedding_model_name)\n",
    "        self.summary_model = pipeline(\"summarization\", model=summary_model_name)\n",
    "\n",
    "\n",
    "    ###############################################################################\n",
    "    # METHODS\n",
    "    ###############################################################################\n",
    "\n",
    "    def query_similar_content(self, query, limit=3):\n",
    "        ''' Generates a query embedding and searches the db for related content '''\n",
    "        \n",
    "        query_embedding = self.embedding_model.encode(query) \n",
    "\n",
    "        results = (\n",
    "            self.db.query(ContentAI, Content)\n",
    "            .join(Content, ContentAI.content_id == Content.content_id)\n",
    "            .order_by(ContentAI.embedding.l2_distance(query_embedding))\n",
    "            .limit(limit)\n",
    "            .all()\n",
    "        )\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "    def insert_embedded_content(self, content_data):\n",
    "        '''\n",
    "        Inserts content into the database if it doesn't exist, summarizes it, and embeds the summary\n",
    "        If any exceptions occur, the transaction will be rolled back\n",
    "        '''\n",
    "        try:\n",
    "\n",
    "            # Check if the url exists in the db already\n",
    "            url = content_data.get(\"url\")\n",
    "            if self._url_exists(url):\n",
    "                return None, None\n",
    "            \n",
    "            # Add content data to the db\n",
    "            content = self._insert_db(Content, content_data)\n",
    "            if content is None: \n",
    "                raise Exception(\"Failed to insert content into the database\")\n",
    "\n",
    "            # Enrich the content by parsing the raw_html. If getting the html fails, default the summary_input to title\n",
    "            summary_input = self._enrich_content(url, content.content_id, self.db)\n",
    "            if summary_input is None:\n",
    "                summary_input = content_data.get(\"title\")\n",
    "\n",
    "            # Use an LLM to summarize the content. If this fails, default to the title for the summary\n",
    "            ai_summary = self._summarize_content(summary_input) \n",
    "            summary = ai_summary if ai_summary else summary_input\n",
    "            if summary is None: \n",
    "                raise Exception(\"Failed to summarize content and/or there is no title\")\n",
    "\n",
    "            # Embed the summary associated with the content ORM\n",
    "            embedding = self.generate_embedding(summary)\n",
    "            if embedding is None: \n",
    "                raise Exception(\"Failed to generate embedding\") \n",
    "\n",
    "            # Insert the embedding data into the db\n",
    "            content_ai_data = {\n",
    "                \"content_id\": content.content_id, \n",
    "                \"ai_summary\": summary, \n",
    "                \"embedding\": embedding\n",
    "            }\n",
    "            content_ai = self._insert_db(ContentAI, content_ai_data)\n",
    "            if content_ai is None: \n",
    "                raise Exception(\"Failed to insert embedding data\") \n",
    "            \n",
    "            # If all steps succeed, then commit transaction to db\n",
    "            self.db.commit()\n",
    "\n",
    "            print(\n",
    "                f\"Created Content ID: {content.content_id},\\n\"\n",
    "                f\"Content AI ID: {content_ai.content_id},\\n\"\n",
    "                f\"Embedding (first 10): {content_ai.embedding[:10]},\\n\"\n",
    "                f\"Summary that was embedded {summary}\\n\\n\"\n",
    "            )\n",
    "\n",
    "            return content, content_ai\n",
    "        \n",
    "        except (SQLAlchemyError, Exception) as e:\n",
    "            self.db.rollback()\n",
    "            print(f\"Error occured in the insert_embedded_content function. Nothing commited to database: {e}\")\n",
    "            return None, None\n",
    "\n",
    "\n",
    "    def generate_embedding(self, text):\n",
    "        ''' Generates an embedding for a piece of text using a Sentence Transformer embedding model '''\n",
    "\n",
    "        try:\n",
    "            return self.embedding_model.encode(text)\n",
    "        except Exception as e: \n",
    "            print(f\"An unexpected error occurred during embedding: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "    ###############################################################################\n",
    "    # HELPER METHODS\n",
    "    ###############################################################################\n",
    "\n",
    "\n",
    "    def _enrich_content(self, url: str, content_id: UUID, db: Session):\n",
    "        response = requests.get(url, timeout=5)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}: failed get request for {url}, defaulting to title for summarization input\")\n",
    "            return None\n",
    "        \n",
    "        raw_html = response.text\n",
    "        metadata = self._extract_metadata_and_body(raw_html)\n",
    "        summary_input = self._build_summary_input(metadata)\n",
    "\n",
    "        print(f\"THE SUMMARY INPUT AFTER ENRICHING IS = {summary_input}\")\n",
    "\n",
    "        return summary_input\n",
    "\n",
    "\n",
    "    def _extract_metadata_and_body(self, html: str) -> dict:\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        \n",
    "        title = soup.title.string.strip() if soup.title else \"\"\n",
    "        description = \"\"\n",
    "        tags = []\n",
    "\n",
    "        for meta in soup.find_all(\"meta\"):\n",
    "            if meta.get(\"name\") == \"description\":\n",
    "                description = meta.get(\"content\", \"\")\n",
    "            if meta.get(\"property\") == \"og:description\":\n",
    "                description = meta.get(\"content\", \"\") or description\n",
    "            if meta.get(\"name\") == \"keywords\":\n",
    "                tags = [tag.strip() for tag in meta.get(\"content\", \"\").split(\",\")]\n",
    "\n",
    "        readable_doc = Document(html)\n",
    "        # html snippet of main content body with boilerplate (nav bars, ads, footers) removed\n",
    "        body_html = readable_doc.summary() \n",
    "        body_text = BeautifulSoup(body_html, \"html.parser\").get_text()\n",
    "\n",
    "        print(f\"The title from soup is: {title}\")\n",
    "\n",
    "        return {\n",
    "            \"title\": title,\n",
    "            \"description\": description,\n",
    "            \"tags\": tags,\n",
    "            \"body_text\": body_text.strip()\n",
    "        }\n",
    "\n",
    "\n",
    "    def _build_summary_input(self, metadata: dict) -> str:\n",
    "        input_parts = []\n",
    "\n",
    "        if metadata[\"title\"]:\n",
    "            input_parts.append(f\"Title: {metadata[\"title\"]}\")\n",
    "        if metadata[\"description\"]:\n",
    "            input_parts.append(f\"Description: {metadata[\"description\"]}\")\n",
    "        if metadata[\"tags\"]:\n",
    "            input_parts.append(f\"Tags: {\", \".join(metadata[\"tags\"])}\")\n",
    "        \n",
    "        '''\n",
    "        Content snippet seems to be messing up the summarizer\n",
    "        The content may not be relavant \n",
    "        Example:\n",
    "            for https://www.lancasterpuppies.com/puppy-search/state/NY?sortBy=prod_all_listings\n",
    "            Content snippet is copyright info\n",
    "            and the summary that gets embeeded is: site logo, Web Layout, and all pictures and text are copyright 2014-2024 by PMG US, LLC.\n",
    "        Commenting put the content snippet seems to help\n",
    "        '''\n",
    "        # snippet = metadata[\"body_text\"][:500]\n",
    "        # input_parts.append(f\"Content Snippet: {snippet}\")\n",
    "\n",
    "        return \"\\n\".join(input_parts)\n",
    "\n",
    "\n",
    "    def _insert_db(self, Data_Model, data):\n",
    "        '''\n",
    "        Takes a data model ORM and inserts data into that table\n",
    "        Returns that db object data\n",
    "        '''\n",
    "        try:\n",
    "            db_data = Data_Model(**data)\n",
    "            self.db.add(db_data)\n",
    "            self.db.flush()     # Flush for content_ai insertion\n",
    "            return db_data\n",
    "        except SQLAlchemyError as e:\n",
    "            self.db.rollback()\n",
    "            print(f\"Error Inserting into {Data_Model.__tablename__}: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "    def _url_exists(self, url):\n",
    "        ''' Checks if a URL already exists in the database '''\n",
    "        if url:\n",
    "            existing_content = self.db.scalar(select(Content).where(Content.url == url))\n",
    "            if existing_content:\n",
    "                print(f\"Content with URL '{url}' already exists. Skipping insertion.\")\n",
    "                return existing_content  \n",
    "        return False\n",
    "    \n",
    "    \n",
    "    def _summarize_content(self, summary_input):\n",
    "        ''' Uses a summary model to get a more detailed summary for the content embeddings '''\n",
    "        \n",
    "        # Debug (TO REMOVE)\n",
    "        print(f\"The summary input being passed to summary model is: {summary_input}\")\n",
    "\n",
    "        # Check if there is input first\n",
    "        if summary_input is None:\n",
    "            return None\n",
    "\n",
    "        try:\n",
    "            input_length = len(self.embedding_model.tokenizer.encode(summary_input)) # Get actual token length\n",
    "            max_length = int(input_length * 0.6)  # Set the max length to about 60 % of input (we can change)\n",
    "            max_length = max(30, min(max_length, 150)) # Ensure the max length is within a reasonable range\n",
    "            summary = self.summary_model(summary_input, max_length=max_length, min_length=15, do_sample=False)[0]['summary_text']\n",
    "            return summary\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during summarization: {e}\")\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The title from soup is: Breaking News, Latest News and Videos | CNN\n",
      "THE SUMMARY INPUT AFTER ENRICHING IS = Title: Breaking News, Latest News and Videos | CNN\n",
      "Description: View the latest news and breaking news today for U.S., world, weather, entertainment, politics and health at CNN.com.\n",
      "Tags: cnn news, daily news, breaking news, news today, current events\n",
      "The summary input being passed to summary model is: Title: Breaking News, Latest News and Videos | CNN\n",
      "Description: View the latest news and breaking news today for U.S., world, weather, entertainment, politics and health at CNN.com.\n",
      "Tags: cnn news, daily news, breaking news, news today, current events\n",
      "Created Content ID: bf401fd7-eec9-456d-a265-277e28adae99,\n",
      "Content AI ID: bf401fd7-eec9-456d-a265-277e28adae99,\n",
      "Embedding (first 10): [-0.00127274 -0.02549458  0.03402197  0.02090326  0.11594051 -0.00539603\n",
      " -0.03234625 -0.02586317 -0.03936379 -0.02048217],\n",
      "Summary that was embedded Tags: cnn news, breaking news, news today, current events, news and breaking news .\n",
      "\n",
      "\n",
      "The title from soup is: Puppies for sale in New York | Lancaster Puppies\n",
      "THE SUMMARY INPUT AFTER ENRICHING IS = Title: Puppies for sale in New York | Lancaster Puppies\n",
      "Description: Find puppies for sale in New York on Lancaster Puppies - The #1 online marketplace to buy and sell puppies in New York.\n",
      "The summary input being passed to summary model is: Title: Puppies for sale in New York | Lancaster Puppies\n",
      "Description: Find puppies for sale in New York on Lancaster Puppies - The #1 online marketplace to buy and sell puppies in New York.\n",
      "Created Content ID: a1ef71cc-7813-4d1f-b09e-05bb5a5d5c10,\n",
      "Content AI ID: a1ef71cc-7813-4d1f-b09e-05bb5a5d5c10,\n",
      "Embedding (first 10): [-0.01824263 -0.08501491  0.06489807 -0.01231944 -0.05576228  0.03060605\n",
      " -0.06183546 -0.02284733 -0.04623424 -0.017433  ],\n",
      "Summary that was embedded Lancaster Puppies is the #1 online marketplace to buy and sell puppies in new york .\n",
      "\n",
      "\n",
      "The title from soup is: LA Fitness | Gym and Fitness Club | Join Today\n",
      "THE SUMMARY INPUT AFTER ENRICHING IS = Title: LA Fitness | Gym and Fitness Club | Join Today\n",
      "Tags: LA Fitness, complimentary pass, free pass, free guest pass, free gym pass, gym trial, free trial membership, gym membership, gym, the fitness club, fitness guest pass, indoor cycling, group fitness classes, fitness studio classes, spin classes, indoor swimming pool, racquetball, gym with basketball, personal training, gym trainer, gyms with classes, gym membership, gym with childcare, gym guest pass, gym near me, personal trainer\n",
      "The summary input being passed to summary model is: Title: LA Fitness | Gym and Fitness Club | Join Today\n",
      "Tags: LA Fitness, complimentary pass, free pass, free guest pass, free gym pass, gym trial, free trial membership, gym membership, gym, the fitness club, fitness guest pass, indoor cycling, group fitness classes, fitness studio classes, spin classes, indoor swimming pool, racquetball, gym with basketball, personal training, gym trainer, gyms with classes, gym membership, gym with childcare, gym guest pass, gym near me, personal trainer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 30, but your input_length is only 29. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=14)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Content ID: 71b07909-64ed-46fd-99fb-383f55477606,\n",
      "Content AI ID: 71b07909-64ed-46fd-99fb-383f55477606,\n",
      "Embedding (first 10): [-0.004375   -0.05000497 -0.02920699  0.00618206 -0.03755658 -0.03490396\n",
      "  0.01428863 -0.12507975 -0.02801919  0.01826911],\n",
      "Summary that was embedded fitness club, fitness guest pass, indoor cycling, group fitness classes, spin classes, indoor swimming pool, racquetball, gym with basketball, personal training, gym trainer, gyms with classes .\n",
      "\n",
      "\n",
      "Error: 403: failed get request for https://www.barnesandnoble.com/, defaulting to title for summarization input\n",
      "The summary input being passed to summary model is: Online Bookstore: Books, NOOK ebooks, Music, Movies & Toys | Barnes & Noble®\n",
      "Created Content ID: 9fd4a34b-ac60-40fe-b814-0c6ba572f781,\n",
      "Content AI ID: 9fd4a34b-ac60-40fe-b814-0c6ba572f781,\n",
      "Embedding (first 10): [-0.00601284 -0.0259262   0.02008257  0.02900521 -0.07970294  0.05311825\n",
      " -0.09713804  0.00011612  0.10316836  0.05342844],\n",
      "Summary that was embedded online bookstore: Books, NOOK ebooks, Music, Movies & Toys | Barnes & Noble®.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 30, but your input_length is only 15. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The title from soup is: Reddit - The heart of the internet\n",
      "THE SUMMARY INPUT AFTER ENRICHING IS = Title: Reddit - The heart of the internet\n",
      "The summary input being passed to summary model is: Title: Reddit - The heart of the internet\n",
      "Created Content ID: b0508818-72f1-4276-98da-008c47c676c9,\n",
      "Content AI ID: b0508818-72f1-4276-98da-008c47c676c9,\n",
      "Embedding (first 10): [-0.05809776  0.02277812  0.03312013 -0.03189891  0.0085295  -0.03829026\n",
      "  0.05035569 -0.06179682  0.09096923 -0.08921135],\n",
      "Summary that was embedded title: reddit . The heart of the internet .\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<__main__.Content at 0x36c074f80>, <__main__.ContentAI at 0x3e7080d70>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = next(get_db())\n",
    "\n",
    "manager = ContentEmbeddingManager(db)\n",
    "\n",
    "# These are fake websites so will get a 404 error\n",
    "# for i, sentence in enumerate(sentences):\n",
    "#     content_data = {\"url\": f\"http://example.com/{i}\", \"title\": f\"Document {i}\"}\n",
    "#     manager.insert_embedded_content(content_data)\n",
    "\n",
    "# Lets try some real websites\n",
    "content_data = {\"url\": f\"https://www.cnn.com/\", \"title\": \"Breaking News, Latest News and Videos | CNN\"}\n",
    "manager.insert_embedded_content(content_data)\n",
    "\n",
    "content_data = {\"url\": f\"https://www.lancasterpuppies.com/puppy-search/state/NY?sortBy=prod_all_listings\", \"title\": \"Puppies for sale in New York | Lancaster Puppies\"}\n",
    "manager.insert_embedded_content(content_data)\n",
    "\n",
    "content_data = {\"url\": f\"https://www.lafitness.com/Pages/default.aspx\", \"title\": \"LA Fitness | Gym and Fitness Club | Join Today\"}\n",
    "manager.insert_embedded_content(content_data)\n",
    "\n",
    "content_data = {\"url\": f\"https://www.barnesandnoble.com/\", \"title\": \"Online Bookstore: Books, NOOK ebooks, Music, Movies & Toys | Barnes & Noble®\"}\n",
    "manager.insert_embedded_content(content_data)\n",
    "\n",
    "# test reddit\n",
    "content_data = {\"url\": f\"https://www.reddit.com/r/PlanetFitnessMembers/comments/1cpi6r9/where_do_you_get_your_workout_clothes/\", \"title\": \"Where Do You Get Your Workout Clothes? : r/PlanetFitnessMembers\"}\n",
    "manager.insert_embedded_content(content_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am querying: pets\n",
      "\n",
      "\n",
      "Similar Content --------------------\n",
      "\n",
      "AI SUMMARY: Lancaster Puppies is the #1 online marketplace to buy and sell puppies in new york .,\n",
      " Content ID: a1ef71cc-7813-4d1f-b09e-05bb5a5d5c10,\n",
      " Title: Puppies for sale in New York | Lancaster Puppies,\n",
      " Embedding (first 10): [-0.01824263 -0.08501491  0.06489807 -0.01231944 -0.05576228  0.03060605\n",
      " -0.06183546 -0.02284733 -0.04623424 -0.017433  ]\n",
      "\n",
      "\n",
      "AI SUMMARY: Tags: cnn news, breaking news, news today, current events, news and breaking news .,\n",
      " Content ID: bf401fd7-eec9-456d-a265-277e28adae99,\n",
      " Title: Breaking News, Latest News and Videos | CNN,\n",
      " Embedding (first 10): [-0.00127274 -0.02549458  0.03402197  0.02090326  0.11594051 -0.00539603\n",
      " -0.03234625 -0.02586317 -0.03936379 -0.02048217]\n",
      "\n",
      "\n",
      "AI SUMMARY: title: reddit . The heart of the internet .,\n",
      " Content ID: b0508818-72f1-4276-98da-008c47c676c9,\n",
      " Title: Where Do You Get Your Workout Clothes? : r/PlanetFitnessMembers,\n",
      " Embedding (first 10): [-0.05809776  0.02277812  0.03312013 -0.03189891  0.0085295  -0.03829026\n",
      "  0.05035569 -0.06179682  0.09096923 -0.08921135]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "db = next(get_db())\n",
    "\n",
    "# Similarity Search\n",
    "query = \"pets\"\n",
    "print(f\"I am querying: {query}\\n\")\n",
    "similar_results = manager.query_similar_content(query, limit=3)\n",
    "\n",
    "print(\"\\nSimilar Content --------------------\\n\")\n",
    "for content_ai, content in similar_results:\n",
    "    print(f\"AI SUMMARY: {content_ai.ai_summary},\\n \"f\"Content ID: {content.content_id},\\n Title: {content.title},\\n Embedding (first 10): {content_ai.embedding[:10]}\\n\\n\")\n",
    "\n",
    "db.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
