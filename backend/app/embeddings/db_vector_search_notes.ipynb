{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create user and set up server\n",
    "\n",
    "// starting Postgres server:\n",
    "* brew services start postgresql\n",
    "\n",
    "// stoping Postgres server:\n",
    "* brew services stop postgresql\n",
    "* brew services list\n",
    "\n",
    "// access CLI\n",
    "* psql postgres\n",
    "\n",
    "// create db\n",
    "* CREATE DATABASE mydb;\n",
    "\n",
    "// create a user and set pw\n",
    "* CREATE USER postgres WITH PASSWORD 'cunytechprep';\n",
    "\n",
    "// grant priv for this user\n",
    "* GRANT ALL PRIVILEGES ON DATABASE mydb TO postgres;\n",
    "\n",
    "// dang they need super user priv had to go back to cli and do this\n",
    "* ALTER USER postgres WITH SUPERUSER;\n",
    "\n",
    "// exit CLI\n",
    "* \\q\n",
    "\n",
    "// access cli for this database\n",
    "* psql -d mydb -U postgres\n",
    "\n",
    "// Enable the pgvector extension, make sure you do superuser priv first\n",
    "* CREATE EXTENSION vector;\n",
    "\n",
    "// verify extension is enabled\n",
    "* \\dx\n",
    "\n",
    "// DONE ------------------------------\n",
    "\n",
    "// TO drop tables\n",
    "* psql -d mydb -U postgres\n",
    "* \\dt\n",
    "* DROP TABLE IF EXISTS table_name CASCADE;\n",
    "\n",
    "DROP TABLE IF EXISTS content CASCADE;\n",
    "DROP TABLE IF EXISTS content_ai CASCADE;\n",
    "DROP TABLE IF EXISTS content_item CASCADE;\n",
    "DROP TABLE IF EXISTS users CASCADE;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SET UP DB/IMPORTS/ENV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, Column, Integer, String, ARRAY, Float, text, TIMESTAMP, ForeignKey\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.dialects.postgresql import UUID as DB_UUID\n",
    "import uuid\n",
    "import os\n",
    "from sqlalchemy.orm import sessionmaker, declarative_base, Session\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel\n",
    "from typing import Optional\n",
    "from uuid import UUID\n",
    "import psycopg2\n",
    "from pgvector.sqlalchemy import Vector\n",
    "from sqlalchemy import select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB URL:  postgresql://postgres:cunytechprep@localhost:5432/mydb\n",
      "Connected\n"
     ]
    }
   ],
   "source": [
    "# Create the sqlachemy engine and connect to our db\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "DATABASE_URL = os.getenv(\"DATABASE_URL\")\n",
    "print(\"DB URL: \", DATABASE_URL)\n",
    "\n",
    "try:\n",
    "    engine = create_engine(DATABASE_URL)\n",
    "    print(\"Connected\")\n",
    "except Exception as e:\n",
    "    print(\"Connection falied: \", e)\n",
    "\n",
    "# Create a session\n",
    "# auto commmit, transactions are not auto commited, need to call session.commit()\n",
    "# auto flush, changes are not automatically written to db before every query, need to call session.flush()\n",
    "# bind to the db engine we created\n",
    "SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n",
    "\n",
    "# Base class for declarative models\n",
    "# Any class that inherits from this will be recognized by SQLAlchemy as a database table\n",
    "Base = declarative_base()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQLAlchemy Data models\n",
    "\n",
    "class ContentAI(Base):\n",
    "    __tablename__ = \"content_ai\"\n",
    "\n",
    "    content_id = Column(DB_UUID, ForeignKey(\"content.content_id\"), primary_key=True)\n",
    "    ai_summary = Column(String, nullable=True)\n",
    "    embedding = Column(Vector(dim=2), nullable=True)\n",
    "    # embedding = Column(String, nullable=True) # pgvector integration may need different type (OLD CODE)\n",
    "\n",
    "    # NEED TO REPLACE DIMENSIONS WITH CORRECT EMBEDDING MODEL\n",
    "    # https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n",
    "    # For example, this transformer has 384 dim dense vector\n",
    "    # I am using vector w/ dim of 2 for now \n",
    "    \n",
    "class ContentItem(Base):\n",
    "    __tablename__ = \"content_item\"\n",
    "\n",
    "    user_id = Column(DB_UUID, ForeignKey(\"users.id\"), primary_key=True)\n",
    "    content_id = Column(DB_UUID, ForeignKey(\"content.content_id\"), primary_key=True)\n",
    "    saved_at = Column(TIMESTAMP, server_default=\"NOW()\")\n",
    "    notes = Column(String, nullable=True)\n",
    "\n",
    "class Content(Base):\n",
    "    __tablename__ = \"content\"\n",
    "\n",
    "    content_id = Column(DB_UUID, primary_key=True, default=uuid.uuid4)\n",
    "    user_id = Column(DB_UUID, ForeignKey(\"users.id\"))\n",
    "    url = Column(String, unique=True, nullable=False)   \n",
    "    title = Column(String, nullable=True)\n",
    "    source = Column(String, nullable=True)\n",
    "    first_saved_at = Column(TIMESTAMP, server_default=\"NOW()\")\n",
    "\n",
    "class User(Base):\n",
    "    __tablename__ = \"users\"\n",
    "\n",
    "    id = Column(DB_UUID, primary_key=True, default=uuid)\n",
    "    email = Column(String, unique=True, nullable=False)\n",
    "    created_at = Column(TIMESTAMP, server_default=\"NOW()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schemas content, these are pydantic schemas for data val and serialization\n",
    "\n",
    "class ContentCreate(BaseModel):\n",
    "    url: str\n",
    "    title: Optional[str] = None\n",
    "    source: Optional[str] = None\n",
    "\n",
    "\n",
    "class ContentRead(ContentCreate):\n",
    "    content_id: UUID\n",
    "\n",
    "    class Config:\n",
    "        from_attributes=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tables created\n"
     ]
    }
   ],
   "source": [
    "# Create tables for the sqlalchemy models defined above, only creates tables that do not exist\n",
    "\n",
    "Base.metadata.create_all(bind=engine)\n",
    "print(\"All tables created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database.py\n",
    "\n",
    "# yield a fresh session per request (FASTAPI), caller to use the session and closes when done w/ session\n",
    "def get_db():\n",
    "    # session instance\n",
    "    db = SessionLocal()  \n",
    "    try:\n",
    "        yield db         \n",
    "    finally:\n",
    "        db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful\n"
     ]
    }
   ],
   "source": [
    "# test connection\n",
    "\n",
    "try:\n",
    "    conn = psycopg2.connect(\n",
    "        dbname=\"mydb\",\n",
    "        user=\"postgres\",\n",
    "        password=\"cunytechprep\",\n",
    "        host=\"localhost\",\n",
    "        port=\"5432\"\n",
    "    )\n",
    "    print(\"Connection successful\")\n",
    "    conn.close()    \n",
    "except Exception as e:\n",
    "    print(\"Connection failed:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEARCHING AND INSERTING EMBEDDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating random dummy embedding for: 'dummy test'\n",
      "Type of return =  <class 'list'>\n",
      "Dummy embedding: [2, 0] (length: 2)\n"
     ]
    }
   ],
   "source": [
    "# Define a dummy embedding model to test inserting vectors and performing similarity searches\n",
    "\n",
    "def generate_dummy_embedding(text):\n",
    "    print(f\"Generating random dummy embedding for: '{text}'\")\n",
    "\n",
    "    # This will be similar to document 1\n",
    "    if text == \"Looking for something similar\":\n",
    "        return [.5, 0]\n",
    "    elif text == \"Document 1\":\n",
    "        return [1, 0]\n",
    "    \n",
    "    # Document 2\n",
    "    return [2, 0]\n",
    "\n",
    "dummy_embedding = generate_dummy_embedding(\"dummy test\")\n",
    "print(\"Type of return = \", type(dummy_embedding))\n",
    "print(f\"Dummy embedding: {dummy_embedding[:10]} (length: {len(dummy_embedding)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following will be given to the LLM to be summarized http://example.com/doc1, Document 1\n",
      "Generating random dummy embedding for: 'Document 1'\n",
      "Dummy embedding: [1, 0] (length: 2)\n",
      "Created Content ID: 140414cf-5e18-4e74-b1d2-c60a2ef6e684, Content AI ID: 140414cf-5e18-4e74-b1d2-c60a2ef6e684, Embedding (first 10): [1. 0.]\n",
      "The following will be given to the LLM to be summarized http://example.com/doc2, Document 2\n",
      "Generating random dummy embedding for: 'Document 2'\n",
      "Dummy embedding: [2, 0] (length: 2)\n",
      "Created Content ID: fcbb049b-438e-4827-9477-b1f3edf2264d, Content AI ID: fcbb049b-438e-4827-9477-b1f3edf2264d, Embedding (first 10): [2. 0.]\n",
      "Content with URL 'http://example.com/doc1' already exists. Skipping insertion.\n"
     ]
    }
   ],
   "source": [
    "# I am loosely defining this function for now and will clean it up\n",
    "\n",
    "def create_content_with_embedding(db, content_data):\n",
    "\n",
    "    # Check if this urls exists\n",
    "    url_to_check = content_data.get(\"url\")\n",
    "    if url_to_check:\n",
    "        existing_content = db.scalar(select(Content).where(Content.url == url_to_check))\n",
    "        if existing_content:\n",
    "            print(f\"Content with URL '{url_to_check}' already exists. Skipping insertion.\")\n",
    "            return existing_content, None  \n",
    "\n",
    "    # Add the content data to the db\n",
    "    content = Content(**content_data)\n",
    "    db.add(content)\n",
    "    db.commit()\n",
    "    db.refresh(content)\n",
    "\n",
    "    # Use an LLM to summarize the url\n",
    "    # TODO -> Need to implement a LLM to summarize before embedding\n",
    "    # Need to figure out what we are giving to LLM, assume for now -> url, title, source(?)\n",
    "    url = content.url\n",
    "    title = content.title\n",
    "    # source = content.source\n",
    "    print(f\"The following will be given to the LLM to be summarized {url}, {title}\")\n",
    "    ai_summary = \"This is a placeholder for ai_summary before embedding\"\n",
    "    \n",
    "    # Embed the ai summary text\n",
    "    # TODO CHANGE TO EMBEDDING MODEL!!!! \n",
    "    embedding_vector = generate_dummy_embedding(title) # CHANGE TO AI SUMMARY\n",
    "    print(f\"Dummy embedding: {embedding_vector[:10]} (length: {len(embedding_vector)})\") # to be removed\n",
    "    content_ai = ContentAI(content_id=content.content_id, embedding=embedding_vector)\n",
    "    db.add(content_ai)\n",
    "    db.commit()\n",
    "    db.refresh(content_ai)\n",
    "    return content, content_ai\n",
    "\n",
    "\n",
    "# Call the generator function on a new sessions, fresh session per request FASTAPI\n",
    "db = next(get_db())\n",
    "\n",
    "content1_data = {\"url\": \"http://example.com/doc1\", \"title\": \"Document 1\"}\n",
    "content1, content_ai1 = create_content_with_embedding(db, content1_data)\n",
    "if content1 and content_ai1:\n",
    "    print(f\"Created Content ID: {content1.content_id}, Content AI ID: {content_ai1.content_id}, Embedding (first 10): {content_ai1.embedding[:10]}\")\n",
    "\n",
    "content2_data = {\"url\": \"http://example.com/doc2\", \"title\": \"Document 2\"}\n",
    "content2, content_ai2 = create_content_with_embedding(db, content2_data)\n",
    "if content2 and content_ai2:\n",
    "    print(f\"Created Content ID: {content2.content_id}, Content AI ID: {content_ai2.content_id}, Embedding (first 10): {content_ai2.embedding[:10]}\")\n",
    "\n",
    "# TEST INSERTING THE SAME URL\n",
    "content3_data = {\"url\": \"http://example.com/doc1\", \"title\": \"Document 1\"}\n",
    "content3, content_ai3 = create_content_with_embedding(db, content3_data)\n",
    "if content3 and content_ai3:\n",
    "    print(\"This should not be printed\")\n",
    "\n",
    "\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating random dummy embedding for: 'Looking for something similar'\n",
      "This query embedding = [0.5, 0]\n",
      "\n",
      "Similar Content:\n",
      "Content ID: 140414cf-5e18-4e74-b1d2-c60a2ef6e684, Title: Document 1, Embedding (first 10): [1. 0.]\n",
      "Content ID: fcbb049b-438e-4827-9477-b1f3edf2264d, Title: Document 2, Embedding (first 10): [2. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Vector class https://github.com/pgvector/pgvector-python/blob/master/pgvector/sqlalchemy/vector.py#L43\n",
    "\n",
    "def find_similar_content(db, query_text, limit=2):\n",
    "    query_embedding = generate_dummy_embedding(query_text)\n",
    "    print(f\"This query embedding = {query_embedding}\")\n",
    "\n",
    "    # Test manually\n",
    "    # query_embedding = [3, 0] # Document 2 shows up first\n",
    "    # query_embedding = [1.2, 0] # Document 1 shows up first\n",
    "\n",
    "    results = db.query(ContentAI, Content) \\\n",
    "        .join(Content, ContentAI.content_id == Content.content_id) \\\n",
    "        .order_by(ContentAI.embedding.l2_distance(query_embedding)) \\\n",
    "        .limit(limit) \\\n",
    "        .all()\n",
    "\n",
    "    return results\n",
    "\n",
    "# Similarity Search\n",
    "db = next(get_db())\n",
    "similar_results = find_similar_content(db, \"Looking for something similar\")\n",
    "print(\"\\nSimilar Content:\")\n",
    "for content_ai, content in similar_results:\n",
    "    print(f\"Content ID: {content.content_id}, Title: {content.title}, Embedding (first 10): {content_ai.embedding[:10]}\")\n",
    "\n",
    "db.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
